---
title: 分类问题中的样本不均衡算法
date: 2018-07-10 16:53:30
tags: [机器学习,非均衡样本,模型训练]
categories: 机器学习
---

最近在做一个房态预测的比赛，比赛具体是对具体客户实现预定时有无房间的预测，官方给定的数据集是标准的非均衡样本。

所以学习了一些分类问题中的样本不均衡算法。
<!-- more -->

1. 根据"The strength of weak learnablility" 方法，该方法是一种boosting算法，它递归地训练三个弱学习器，然后将这三个弱学习器结合起形成一个强的学习器。算法流程如下：
* 首先使用原始数据集训练第一个学习器L1。
* 然后使用50%在L1中学习正确和50%学习错误的样本训练得到学习器L2，即从L1中学习错误的样本集和学习正确的样本集中，循环采样，各占一半。
* 接着使用L1和L2不一致的那些样本去训练得到学习器L3。
* 最后，使用投票方式来预测测试集。
应用到具体问题中，假设是一个二分类问题，大类为true类，流程如下：
* 使用60%原始样本来训练L1
* 用L1对剩下带标签的原始样本进行预测，取分类正确和错误的样本各50%，生成平衡的L2的样本集
* 用其中60%的样本来训练L2，对剩下的样本进行预测
* 对L1与L2分类不一样的样本进行训练来得到L3
* 结合L1，L2，L3，采用投票的方式来决定分类结果
代码实现如下：
``` python
def model_L1(data):
    label = 'LABEL'
    IDcol = 'PERSONID'
    predictors = [x for x in data.columns if x not in [label,IDcol]]
    X_train, X_test, y_train, y_test = train_test_split(data[predictors],data[label],test_size=0.4,random_state=2018)

    model = lgb.LGBMClassifier(objective='binary',metric='AUC',num_leaves=90,depth=8,learning_rate=0.01,
                               colsample_bytree=0.8,n_estimators=2000,seed=2018,subsample=0.9,boosting='rf',
                               boosting_type='gbdt',reg_alpha=0.0,reg_lambda=0.7,bagging_fraction=0.7,
                               bagging_freq=1)
    model.fit(X_train,y_train,eval_set=[(X_test,y_test)],eval_metric='AUC',early_stopping_rounds=0,verbose=False)
    prediction_L1 = model.predict(X_test)
    prediction1 = model.predict_proba(X_test)
    area1 = auc_score(y_test,prediction1[:,1])
    print('the test score is:{}'.format(area1))
    print(classification_report(y_test,prediction_L1,target_names=["noroom","haveroom"]))
#    print('the y_test is:{},shape is:{}'.format(y_test,y_test.shape))
    prediction2 = model.predict_proba(X_train)
    area2 = auc_score(y_train,prediction2[:,1])
    print('the train score is:{}'.format(area2))
    
    i = 0
    wrong_index = [] # 得到L1学习器预测错误的所有样本index
    for j in y_test.index:
        if (prediction_L1[i] != y_test[j]):
            wrong_index.append(j)
            i += 1
    print("the num of wrong predictions is:{}".format(i))
    
    right_index = [] # 取与预测错误的index数目相同的预测正确的样本index
    num = 0
    for index in y_test.index:
        if index not in wrong_index:
            right_index.append(index)
            num += 1 
            if num == i: # 82915需要根据新数据集进行更改
                break
    print("the num of right index is:{}".format(len(right_index)))
#    for num in range(82915): # 82915需要根据新数据集进行更改
#        right_index.append(right_indexs[num])
    total_index = wrong_index + right_index
    return model, predictors,total_index,prediction_L1

def model_L2(data):
    label = 'LABEL'
    IDcol = 'PERSONID'
    predictors = [x for x in data.columns if x not in [label,IDcol]]
    X_train, X_test, y_train, y_test = train_test_split(data[predictors],data[label],test_size=0.4,random_state=2018)
    model = lgb.LGBMClassifier(objective='binary',metric='AUC',num_leaves=90,depth=8,learning_rate=0.01,
                               colsample_bytree=0.8,n_estimators=2000,seed=2018,subsample=0.9,boosting='rf',
                               boosting_type='gbdt',reg_alpha=0.0,reg_lambda=0.0,bagging_fraction=0.7,
                               bagging_freq=1)
    model.fit(X_train,y_train,eval_set=[(X_test,y_test)],eval_metric='AUC',early_stopping_rounds=0,verbose=False)
    prediction_L2 = model.predict(X_test)
    L3_index = []
    i = 0
    for index in y_test.index:
        if(prediction_L2[i] != y_test[index]):
            L3_index.append(index)
            i += 1
    return model,L3_index,prediction_L2

def model_L3(data):
    label = 'LABEL'
    IDcol = 'PERSONID'
    predictors = [x for x in data.columns if x not in [label,IDcol]]
    X_train,X_test,y_train,y_test = train_test_split(data[predictors],data[label],test_size=0.1,random_state=2018)
    model = lgb.LGBMClassifier(objective='binary',metric='AUC',num_leaves=90,depth=8,learning_rate=0.01,
                               colsample_bytree=0.8,n_estimators=2000,seed=2018,subsample=0.9,boosting='rf',
                               boosting_type='gbdt',reg_alpha=0.0,reg_lambda=0.0,bagging_fraction=0.7,
                               bagging_freq=1)
    model.fit(X_train,y_train,eval_set=[(X_test,y_test)],eval_metric='AUC',early_stopping_rounds=3000,verbose=False)
    return model    

model_L1,predictors,L2_index,prediction_L1 = model_L1(train)
#train =train.reset_index()
data_L2 = train[train.index.isin(L2_index)]
#data_L2 = data_L2.set_index('orderid')
model_L2,L3_index,prediction_L2 = model_L2(data_L2)
data_L3 = train[train.index.isin(L3_index)]
#data_L3 = data_L3.set_index('orderid')
model_L3 = model_L3(data_L3)

X_train, X_test, y_train, y_test = train_test_split(train[predictors],train['LABEL'],test_size=0.4,random_state=2018)

label = []
L1_result = model_L1.predict_proba(X_test)[:,1]
L2_result = model_L2.predict_proba(X_test)[:,1]
L3_result = model_L3.predict_proba(X_test)[:,1]
for i in range(len(X_test)):   
    if((L1_result[i] >= 0.5) and (L2_result[i] >= 0.5) and (L3_result[i] >= 0.5)) or ((L1_result[i] < 0.5) and (L2_result[i] < 0.5) and (L3_result[i] < 0.5)):
        label.append((L1_result[i]+L2_result[i]+L3_result[i])/3.0)
    elif ((L1_result[i] >= 0.5) and (L2_result[i] >= 0.5) and (L3_result[i] < 0.5)) or ((L1_result[i] < 0.5) and (L2_result[i] < 0.5) and (L3_result[i] >= 0.5)):
        label.append((L1_result[i]+L2_result[i])/2.0)
    elif ((L1_result[i] >= 0.5) and (L2_result[i] < 0.5) and (L3_result[i] < 0.5)) or ((L1_result[i] < 0.5) and (L2_result[i] >= 0.5) and (L3_result[i] >= 0.5)):
        label.append((L2_result[i]+L3_result[i])/2.0)
    elif ((L1_result[i] >= 0.5) and (L2_result[i] < 0.5) and (L3_result[i] >= 0.5)) or ((L1_result[i] < 0.5) and (L2_result[i] >= 0.5) and (L3_result[i] < 0.5)):
        label.append((L1_result[i]+L3_result[i])/2.0)
#    if((L1_result[i] < 0.5) and (L2_result[i] >= 0.5) and (L3_result[i] >= 0.5)):
#        noroom.append((L2_result[i]+L3_result[i])/2.0)
#    else:
#        noroom.append((L1_result[i]+L2_result[i]+L3_result[i])/3.0)
```

## 以下方法会破坏某些类的样本分布

2. 在训练模型时，可以增加小类样本的权重，降低大类样本的权重，从而使得分类器将重点集中在小类样本上。（这种方法其实改变了原始数据集的样本分布，得到的模型性能甚至会变差）
开始时可以设置每个类别的权重为样本个数比例的倒数，然后使用过采样方法进行调优。

3. 设大类样本的个数是小类样本的L倍，那么在随机梯度下降算法中，每次遇到一个小类样本进行训练时，训练L次。

4. 将大类样本划分到L个聚类，然后训练L个分类器，每个分类器使用大类中的一个簇和所有的小类样本进行训练。最后采用L个分类器投票的方法进行分类，如果是回归问题，则采用平均值。

5. 设小类中有N个样本，将大类聚类成N个簇，然后用每个簇的中心组成N个样本，和小类中的所有样本一起训练。