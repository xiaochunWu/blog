---
title: 梯度提升树(GBDT)概述及sklearn调参
date: 2018-06-11 20:03:47
tags: [机器学习,树模型]
categories: 机器学习
---

# GBDT概述

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>   GBDT隶属于Boosting，但不同于Adaboost，GBDT限定了弱学习器只能使用CART回归树模型。</p>

# 1.GBDT原理概述
<p>　　　　在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是$f_{t-1}(x)$, 损失函数是$L(y,f_{t-1}(x))$, 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器$h_t(x)$，让本轮的损失损失$L(y,f_{t}(x) =L(y,f_{t-1}(x)+ h_t(x))$最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。</p>
<p>　　　　GBDT的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。</p>
<p>　　　　从上面的例子看这个思想还是蛮简单的，但是有个问题是这个损失的拟合不好度量，损失函数各种各样，怎么找到一种通用的拟合方法呢？</p>
<!-- more -->


# 2. GBDT的负梯度拟合
<p>　　　　在上一节中，我们介绍了GBDT的基本思路，但是没有解决损失函数拟合方法的问题。针对这个问题，大牛Freidman提出了用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。第t轮的第i个样本的损失函数的负梯度表示为$$r_{ti} = -\bigg[\frac{\partial L(y_i, f(x_i)))}{\partial f(x_i)}\bigg]_{f(x) = f_{t-1}\;\; (x)}$$</p>
<p>　　　　利用$(x_i,r_{ti})\;\; (i=1,2,..m)$,我们可以拟合一颗CART回归树，得到了第t颗回归树，其对应的叶节点区域$R_{tj}, j =1,2,..., J$。其中J为叶子节点的个数。</p>
<p>　　　　针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的的输出值$c_{tj}$如下：$$c_{tj} = \underbrace{arg\; min}_{c}\sum\limits_{x_i \in R_{tj}} L(y_i,f_{t-1}(x_i) +c)$$</p>
<p>　　　　这样我们就得到了本轮的决策树拟合函数如下：$$h_t(x) = \sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$$</p>
<p>　　　　从而本轮最终得到的强学习器的表达式如下：$$f_{t}(x) = f_{t-1}(x) + \sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$$</p>
<p>　　　　通过损失函数的负梯度来拟合，我们找到了一种通用的拟合损失误差的办法，这样无轮是分类问题还是回归问题，我们通过其损失函数的负梯度的拟合，就可以用GBDT来解决我们的分类回归问题。区别仅仅在于损失函数不同导致的负梯度不同而已。</p>

# 3.GBDT回归算法
<p>　　　　好了，有了上面的思路，下面我们总结下GBDT的回归算法。为什么没有加上分类算法一起？那是因为分类算法的输出是不连续的类别值，需要一些处理才能使用负梯度，我们在下一节讲。</p>
<p>　　　　输入是训练集样本$T=\{(x_,y_1),(x_2,y_2), ...(x_m,y_m)\}$， 最大迭代次数T, 损失函数L。</p>
<p>　　　　输出是强学习器f(x)</p>
<p>　　　　1) 初始化弱学习器$$f_0(x) = \underbrace{arg\; min}_{c}\sum\limits_{i=1}^{m}L(y_i, c)$$</p>
<p>　　　　2) 对迭代轮数t=1,2,...T有：</p>
<p>　　　　　　a)对样本i=1,2，...m，计算负梯度$$r_{ti} = -\bigg[\frac{\partial L(y_i, f(x_i)))}{\partial f(x_i)}\bigg]_{f(x) = f_{t-1}\;\; (x)}$$</p>
<p>　　　　　　b)利用$(x_i,r_{ti})\;\; (i=1,2,..m)$, 拟合一颗CART回归树,得到第t颗回归树，其对应的叶子节点区域为$R_{tj}, j =1,2,..., J$。其中J为回归树t的叶子节点的个数。</p>
<p>　　　　　　c) 对叶子区域j =1,2,..J,计算最佳拟合值$$c_{tj} = \underbrace{arg\; min}_{c}\sum\limits_{x_i \in R_{tj}} L(y_i,f_{t-1}(x_i) +c)$$</p>
<p>　　　　　　d) 更新强学习器$$f_{t}(x) = f_{t-1}(x) + \sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$$</p>
<p>　　　　3) 得到强学习器f(x)的表达式$$f(x) = f_T(x) =f_0(x) + \sum\limits_{t=1}^{T}\sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$$</p>

# 4. GBDT分类算法
<p>　　　　这里我们再看看GBDT分类算法，GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。</p>
<p>　　　　为了解决这个问题，主要有两个方法，一个是用指数损失函数，此时GBDT退化为Adaboost算法。另一种方法是用类似于逻辑回归的对数似然损失函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。本文仅讨论用对数似然损失函数的GBDT分类。而对于对数似然损失函数，我们又有二元分类和多元分类的区别。</p>

## 4.1 二元GBDT分类算法
<p>　　　　对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数为：$$L(y, f(x)) = log(1+ exp(-yf(x)))$$</p>
<p>　　　　其中$y \in\{-1, +1\}$。则此时的负梯度误差为$$r_{ti} = -\bigg[\frac{\partial L(y, f(x_i)))}{\partial f(x_i)}\bigg]_{f(x) = f_{t-1}\;\; (x)} = y_i/(1+exp(y_if(x_i)))$$</p>
<p>　　　　对于生成的决策树，我们各个叶子节点的最佳残差拟合值为$$c_{tj} = \underbrace{arg\; min}_{c}\sum\limits_{x_i \in R_{tj}} log(1+exp(-y_i(f_{t-1}(x_i) +c)))$$</p>
<p>　　　　由于上式比较难优化，我们一般使用近似值代替$$c_{tj} =\sum\limits_{x_i \in R_{tj}}r_{ti}\bigg /\sum\limits_{x_i \in R_{tj}}|r_{ti}|(1-|r_{ti}|)$$</p>
<p>　　　　除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，二元GBDT分类和GBDT回归算法过程相同。</p>

## 4.2 多元GBDT分类算法
<p>　　　　多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假设类别数为K，则此时我们的对数似然损失函数为：$$L(y, f(x)) = - \sum\limits_{k=1}^{K}y_klog\;p_k(x)$$</p>
<p>　　　　其中如果样本输出类别为k，则$y_k=1$。第k类的概率$p_k(x)$的表达式为：$$p_k(x) = exp(f_k(x)) \bigg / \sum\limits_{l=1}^{K}exp(f_l(x))$$</p>
<p>　　　　集合上两式，我们可以计算出第$t$轮的第$i$个样本对应类别$l$的负梯度误差为$$r_{til} =-\bigg[\frac{\partial L(y_i, f(x_i)))}{\partial f(x_i)}\bigg]_{f_k(x) = f_{l, t-1}\;\; (x)} = y_{il} - p_{l, t-1}(x_i)$$</p>
<p>　　　　观察上式可以看出，其实这里的误差就是样本$i$对应类别$l$的真实概率和$t-1$轮预测概率的差值。</p>
<p>　　　　对于生成的决策树，我们各个叶子节点的最佳残差拟合值为$$c_{tjl} = \underbrace{arg\; min}_{c_{jl}}\sum\limits_{i=0}^{m}\sum\limits_{k=1}^{K}L(y_k, f_{t-1, l}(x) + \sum\limits_{j=0}^{J}c_{jl} I(x_i \in R_{tj}))$$</p>
<p>　　　　由于上式比较难优化，我们一般使用近似值代替$$c_{tjl} = \frac{K-1}{K} \; \frac{\sum\limits_{x_i \in R_{tjl}}r_{til}}{\sum\limits_{x_i \in R_{til}}|r_{til}|(1-|r_{til}|)}$$</p>
<p>　　　　除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同。</p>

# 5. GBDT常用损失函数
<p>　　　　这里我们再对常用的GBDT损失函数做一个总结。</p>
<p>　　　　对于分类算法，其损失函数一般有对数损失函数和指数损失函数两种:</p>
<p>　　　　a) 如果是指数损失函数，则损失函数表达式为$$L(y, f(x)) = exp(-yf(x))$$</p>
<p>　　　　其负梯度计算和叶子节点的最佳残差拟合参见Adaboost原理篇。</p>
<p>　　　　b)如果是对数损失函数，分为二元分类和多元分类两种，参见4.1节和4.2节。</p>
<p>　　　　</p>
<p>　　　　对于回归算法，常用损失函数有如下4种:</p>
<p>　　　　a)均方差，这个是最常见的回归损失函数了$$L(y, f(x)) =(y-f(x))^2$$</p>
<p>　　　　b)绝对损失，这个损失函数也很常见$$L(y, f(x)) =|y-f(x)|$$</p>
<p>　　　　　　对应负梯度误差为：$$sign(y_i-f(x_i))$$</p>
<p>　　　　c)Huber损失，它是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。损失函数如下：</p>
<p>$$L(y, f(x))=\begin{cases}\frac{1}{2}(y-f(x))^2 \qquad {|y-f(x)| \leq \delta}\\ \delta(|y-f(x)| - \frac{\delta}{2}) \qquad {|y-f(x)| \geq \delta} \end{cases}$$</p>
<p>　　　　对应的负梯度误差为：</p>
<p>$$r(y_i, f(x_i))= \begin{cases} y_i-f(x_i) \qquad {|y_i-f(x_i)| \leq \delta}\\ \delta sign(y_i-f(x_i)) \qquad {|y_i-f(x_i)| \geq\ \delta} \end{cases}$$</p>
<p>　　　　d) 分位数损失。它对应的是分位数回归的损失函数，表达式为$$L(y, f(x)) =\sum\limits_{y \geq f(x)}\theta|y - f(x)| + \sum\limits_{y   \leq f(x)}(1-\theta)|y - f(x)|$$</p>
<p>　　　　　　其中$\theta$为分位数，需要我们在回归前指定。对应的负梯度误差为：</p>
<p>$$r(y_i, f(x_i))= \begin{cases} \theta \qquad { y_i \geq f(x_i)}\\ \theta - 1 \qquad {y_i \leq f(x_i) } \end{cases}$$</p>
<p>　　　　对于Huber损失和分位数损失，主要用于健壮回归，也就是减少异常点对损失函数的影响。</p>

# 6. GBDT的正则化
<p>　　　　和Adaboost一样，我们也需要对GBDT进行正则化，防止过拟合。GBDT的正则化主要有三种方式。</p>
<p>　　　　第一种是和Adaboost类似的正则化项，即步长(learning rate)。定义为$\nu$,对于前面的弱学习器的迭代$$f_{k}(x) = f_{k-1}(x) + h_k(x) $$</p>
<p>　　　　如果我们加上了正则化项，则有$$f_{k}(x) = f_{k-1}(x) + \nu h_k(x) $$</p>
<p>　　　　$\nu$的取值范围为$0 \leq \nu \leq 1 $。对于同样的训练集学习效果，较小的$\nu$意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。</p>
<p></p>
<p>　　　　第二种正则化的方式是通过子采样比例（subsample）。取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间。</p>
<p>　　　　使用了子采样的GBDT有时也称作随机梯度提升树(Stochastic Gradient Boosting Tree, SGBT)。由于使用了子采样，程序可以通过采样分发到不同的任务去做boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点。</p>
<p></p>
<p>　　　　第三种是对于弱学习器即CART回归树进行正则化剪枝。在决策树原理篇里我们已经讲过，这里就不重复了。</p>

# 7. GBDT小结
<p>　　　　由于GBDT的卓越性能，只要是研究机器学习都应该掌握这个算法，包括背后的原理和应用调参方法。目前GBDT的算法比较好的库是xgboost。当然scikit-learn也可以。</p>
<p>　　　　最后总结下GBDT的优缺点。</p>
<p>　　　　GBDT主要的优点有：</p>
<p>　　　　1) 可以灵活处理各种类型的数据，包括连续值和离散值。</p>
<p>　　　　2) 在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。</p>
<p>　　　　3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。</p>
<p>　　　　GBDT的主要缺点有：</p>
<p>　　　　1)由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。</p>
<p></p>


# 8.GBDT sklearn调参
## 8.1 GBDT类Boosting框架参数
<p>　　　　1)<strong>n_estimators</strong>: 弱学习器的最大迭代次数，或者说最大的弱学习器的个数。太小，容易欠拟合，太大，容易过拟合。默认是100。实际调参过程中常常将n_estimators和learning_rate一起考虑</p>
<p>　　　　2)<strong>learning_rate</strong>: 即每个弱学习器的权重缩减系数$\nu$，也称作步长，在原理篇的正则化章节我们也讲到了，加上了正则化项，我们的强学习器的迭代公式为$f_{k}(x) = f_{k-1}(x) + \nu h_k(x)$。$\nu$的取值范围为$0 \leq \nu \leq 1 $。对于同样的训练集拟合效果，较小的$\nu$意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。所以这两个参数n_estimators和learning_rate要一起调参。一般来说，可以从一个小一点的$\nu$开始调参，默认是1。</p>
<p>　　　　3)<strong>subsample</strong>: 即我们在原理篇的正则化章节讲到的子采样，取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间，默认是1.0，即不使用子采样。</p>
<p>　　　　4)<strong>init</strong>: 即我们的初始化的时候的弱学习器，拟合对应原理篇里面的$f_{0}(x)$，如果不输入，则用训练集样本来做样本集的初始化分类回归预测。否则用init参数提供的学习器做初始化分类回归预测。一般用在我们对数据有先验知识，或者之前做过一些拟合的时候，如果没有的话就不用管这个参数了。</p>
<p>　　　　5)<strong>loss: </strong>即我们GBDT算法中的损失函数。分类模型和回归模型的损失函数是不一样的。</p>
<p>　　　　　　对于分类模型，有对数似然损失函数"deviance"和指数损失函数"exponential"两者输入选择。默认是对数似然损失函数"deviance"。在原理篇中对这些分类损失函数有详细的介绍。一般来说，推荐使用默认的"deviance"。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数等于把我们带到了Adaboost算法。</p>
<p>　　　　　　对于回归模型，有均方差"ls",绝对损失"lad", Huber损失"huber"和分位数损失“quantile”。默认是均方差"ls"。一般来说，如果数据的噪音点不多，用默认的均方差"ls"比较好。如果是噪音点较多，则推荐用抗噪音的损失函数"huber"。而如果我们需要对训练集进行分段预测的时候，则采用“quantile”。</p>
<p>　　　　6)<strong>alpha：</strong>这个参数只有GradientBoostingRegressor有，当我们使用Huber损失"huber"和分位数损失“quantile”时，需要指定分位数的值。默认是0.9，如果噪音点较多，可以适当降低这个分位数的值。</p>

## 8.2 GBDT类弱学习器参数
<p>　　　　这里我们再对GBDT的类库弱学习器的重要参数做一个总结。由于GBDT使用了CART回归决策树，因此它的参数基本来源于决策树类，也就是说，和DecisionTreeClassifier和DecisionTreeRegressor的参数基本类似。如果你已经很熟悉决策树算法的调参，那么这一节基本可以跳过。不熟悉的朋友可以继续看下去。</p>
<p>　　　　1)划分时考虑的最大特征数<strong>max_features</strong>:可以使用很多种类型的值，默认是"None",意味着划分时考虑所有的特征数；如果是"log2"意味着划分时最多考虑$log_2N$个特征；如果是"sqrt"或者"auto"意味着划分时最多考虑$\sqrt{N}$个特征。如果是整数，代表考虑的特征绝对数。如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。一般来说，如果样本特征数不多，比如小于50，我们用默认的"None"就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。</p>
<p>　　　　2)决策树最大深度<strong>max_depth</strong>:默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。</p>
<p>　　　　3)内部节点再划分所需最小样本数<strong>min_samples_split</strong>:这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。</p>
<p>　　　　4)叶子节点最少样本数<strong>min_samples_leaf</strong>:这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。</p>
<p>　　　　5）叶子节点最小的样本权重和<strong>min_weight_fraction_leaf</strong>：这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。</p>
<p>　　　　6)最大叶子节点数<strong>max_leaf_nodes</strong>:通过限制最大叶子节点数，可以防止过拟合，默认是"None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。</p>
<p>　　　　7)节点划分最小不纯度<strong>min_impurity_split:</strong>这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点。即为叶子节点。一般不推荐改动默认值1e-7。</p>
