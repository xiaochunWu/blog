---
title: 天池美年大健康比赛总结-30/3183
date: 2018-06-07 20:26:58
tags: [机器学习, 数据预处理]
categories: 机器学习
---
# 写在前面

​	严格意义上来说，这是我第一次参加机器学习方面的比赛，先不管结果如何，把自己平时所学的算法、tricks应用在实际问题中并且得到一定产出是一件非常有快感的事情。本次比赛历时2个月（2018.4.10-2018.6.7），是我打过的周期最长的一次比赛了，最终拿到了第一赛季82/3151，第二赛季30/3183的成绩。关于成绩，我想说的是，如果看百分比，第二赛季的成绩其实是个还不错的成绩，但是看到有很多第一次比赛的小伙伴拿了20多名，作为一个研一的“老年人”心里多少有点不甘吧。但是这个结果我应该很快就可以接受，重要的是在比赛的这两个月所学到的东西，向前看。岁月不饶人，我又何曾饶过岁月。

<!-- more -->

# 初赛

## 题目

​	根据赛题所给的5万名患者的体检数据，预测患者的舒张压、收缩压、血清甘油三酯、血清低密度脂蛋白、血清高密度脂蛋白 共5项指标。具体的题目和数据集见：[我是题目](https://tianchi.aliyun.com/competition/introduction.htm?spm=5176.11409106.5678.1.4678560cho1fKX&raceId=231654)

开源方案：

初赛12名：[初赛12名方案](https://github.com/yzbdt/tianchi_meinian_rank12_1st_season)

初赛第1名：[初赛第1名方案](<https://github.com/RobinSeaside/Solution-meinian-disease-risk-prediction-1st-in-round1-14th-in-round2> )

## 数据预处理

数据集描述：

数据集大小：300+MB, 800万条*3列 样本数据

数据内容：vid（字符串）、table_id（数字）、field_results（中文、数字混合）

### step1 数据集转换 8,000,000*3 -> 50000*600

​	这一步主要是用了pandas 的pivot()函数，把长数据转换成宽数据。这里会涉及到一个问题，当数据的vid、table_id 相同而field_result 不同（也就是某人同一个项目检查了多次）时，是取field_result 的均值还是取某一个值？我们选择的做法是只取第一次出现的值（值得探讨，这个方案有问题，但是如何优化呢？）

```python
def PreProc(data):   # only for digit , do not used in word
    data = data.drop_duplicates(subset=['vid','table_id'])
    print 'drop_duplicates data shape : {}'.format(data.shape)
    featMat = data.pivot('vid','table_id','field_results')
    print 'drop_duplicates featMat shape : {}'.format(featMat.shape)
    return featMat
```

### step2 数字特征提取

​	数据集转换后发现特征可以大致分为4类：纯数字特征、短文本特征（正常/异常、未见异常/轻度异常）、长文本特征（一般是造影检查的诊断结果，语句格式比较固定，不同患者略有不同）、数字文本混合特征。首先对数字特征进行提取。我们的做法是把特征中所有的数字都提取出来，如果遇到数字文本混合特征，需要该特征的数字样本数大于100。看了初赛第一名的开源代码，他们的做法是数字样本数大于总样本数的50%时，认为是数字特征。这一步主要用了正则表达式和pandas的字符串处理str.extract()方法。

```python
def DigitDetect(featMat):
    for i in featMat.columns:
        featMat[i] = featMat[i].str.extract('(^(\+|\-)?\d+\.?\d*)',expand=False)
    digitOnly = featMat
    digitOnly.dropna(axis=1,how='all',inplace=True)
    print 'digitOnly shape : {}'.format(digitOnly.shape)
    table_id_digitFiltered = digitOnly.count()[digitOnly.count()>100].index
    digitFiltered = digitOnly[table_id_digitFiltered]
    digitFiltered = digitFiltered.fillna('-1')
    print 'digitFiltered shape : {}'.format(digitFiltered.shape)
    return digitFiltered  
```

### step3 短文本特征处理

短文本特征是比较好处理的一类特征，可以直接用labelencoder的方法将文本编码加入模型。这里主要用到的是正则表达式和sklearn里面的labelencoder()思想。

```python
def Feat0421(dataFeatureMat):
    dataFeatureMat['0421'] = dataFeatureMat['0421'].str.replace('(不齐)','(杂乱)')
    dataFeatureMat['0421_HR']='999'
    regular = np.where(dataFeatureMat['0421'].str.contains('(整齐|齐)'))[0]
    dataFeatureMat['0421_HR'].iloc[regular] = '0'
    irregular = np.where(dataFeatureMat['0421'].str.contains('(杂乱|过|早搏)'))[0]
    dataFeatureMat['0421_HR'].iloc[irregular] = '1'
    del dataFeatureMat['0421']
    return dataFeatureMat
```

### step4 长文本特征处理

​	长文本特征包含了丰富的信息，但是也较难处理，由于团队没有NLP方面的积累，这部分工作很难开展。一般来说这部分信息的处理有2种方案，第一，暴力正则，优点是可以准确提取出人类正确理解的信息，缺点是极其浪费体力。第二，word2vector（词向量）、tf-idf（词频统计）等nlp算法，优点是可以拓展nlp方面的知识，写起来不像正则那么麻烦，缺点是这些算法大多关注的是词语之间的关联度，并不能真正理解词语的含义，比如对于某项检查而言，人们最关心的就是结果正常还是不正常，这个信息我们可以轻而易举的从[正常，异常，不正常，轻度异常，未见异常，...]这些词语中区分出来，但是nlp算法找到的却是这些词语之间的关联度，“正常”和“异常”的关联度在算法的视角里可能很高，“正常”和“未见异常”的关联度在算法的视角里可能很低（个人对nlp算法的理解不深，仅对大致看过的tf-idf、word2vec方法而言）。基于这些，我们选择了暴力正则的方法处理长文本特征。

```python
def deal0114(data):
    data['0114'] = data['0114'].str.replace('^\s.*','999')
    data['0114'] = data['0114'].str.replace('.*强回声.*','11')        
    data['0114'] = data['0114'].str.replace('.*无回声.*','10')        
    data['0114'] = data['0114'].str.replace('.*高回声.*','9')        
    data['0114'] = data['0114'].str.replace('.*弱回声.*','8')        
    data['0114'] = data['0114'].str.replace('.*低回声.*','7')        
    data['0114'] = data['0114'].str.replace('.*絮状回声.*','6')                
    data['0114'] = data['0114'].str.replace('.*光团.*','5')        
    data['0114'] = data['0114'].str.replace('.*光斑.*','4')        
    data['0114'] = data['0114'].str.replace('.*声影.*','3')        
    data['0114'] = data['0114'].str.replace('.*可见.*','2')        
    data['0114'] = data['0114'].str.replace('.*附着.*','2')        
    data['0114'] = data['0114'].str.replace('.*探及.*','2')
    data['0114'] = data['0114'].str.replace('.*建议.*','2')        
    data['0114'] = data['0114'].str.replace('.*厚约.*','2')        
    data['0114'] = data['0114'].str.replace('.*mm.*','2')        
    data['0114'] = data['0114'].str.replace('.*cm.*','2')        
    data['0114'] = data['0114'].str.replace('.*毛糙.*','1')        
    data['0114'] = data['0114'].str.replace('.*欠光滑.*','1')        
    data['0114'] = data['0114'].str.replace('.*不光整.*','1')        
    data['0114'] = data['0114'].str.replace('.*欠光整.*','1')        
    data['0114'] = data['0114'].str.replace('.*胆囊大小、形态正常，囊壁光整，囊腔内透声好，胆总管无扩张.*','0') 
    data['0114'] = data['0114'].str.replace('.*胆.*','0') 
    data['0114'] = data['0114'].str.replace('.*未显示.*','0') 
    data['0114'] = data['0114'].str.replace('^0\\s*','0')
    data['0114'] = data['0114'].str.replace('^2\\s*','2')
    data['0114'] = data['0114'].str.replace('^0.*','0')
    data['0114'] = data['0114'].str.replace('^2.*','2')
    data['0114'] = data['0114'].str.replace('^3.*','3')
    data['0114'] = data['0114'].str.replace('^0\n.*','0')
    data['0114'] = data['0114'].str.replace('^2\n.*','2')
    data['0114'] = data['0114'].str.replace('^2\n.*','2')
    data['0114'] = data['0114'].str.replace('^1\n.*','1')
    data['0114'] = data['0114'].str.replace('^999\n.*','999')
    return data
```

​	可以看到，仅对一个长文本特征的处理代码就这么多，而且基本都是是重复的操作。看了初赛第一名的代码发现他们也采取了类似的方案，不同的是他们做了一些特征交叉，而我们没有找到合适的特征交叉的方案（脱敏的特征，不知道如何交叉，看过一个遍历特征做交叉的方案，类似于特征筛选中的过滤式特征筛选，但是我们的特征太多了，遍历代价无法承受，最终放弃）。我们唯一做交叉的是对几个第二性征检查的特征做了交叉，得到性别特征，后面会提到。

### step 5 缺失值处理

​	这是一个比较重要的环节，因为数据的缺失值非常多，有些特征的缺失值达到70%以上。常见的缺失值处理方案有3种：1.根据已知数据补全缺失值，一般按众数或者平均值补全，具体用什么指标需要结合特征的分布进行判断，例如，如果特征分布是一个标准正态分布，那么用均值补全即可，如果是一个长尾分布（不均匀分布），用众数补全较合理。2.若有缺失值的特征数量较少，可以将缺失值当做label，做一个模型来对缺失值进行预测补全。3.用-1或者999对缺失值进行补全。我们采用的是第3个方案，因为特征多，缺失的特征也多，逐一查看特征的分布不是很现实，时间不够（当然也是自己没安排好）。一开始处理的时候还给每列特征加了一个dummy variance，表征是否缺失，后来发现至少对于树模型而言，这种做法完全没有必要，因为树模型受稀疏特征的影响会比较大，添加很多dummy variance后，特征矩阵会变得很稀疏，事实也证明没有dummy variance的效果要更好。所以最终，我们直接用-1 对缺失值进行补全。

### step 6 特征工程

​	到了这一步，我们终于得到了一个适用于机器学习的数据集了，接下来就是特征工程了。特征工程的方法有3种，过滤式特征选择、包裹式特征选择、嵌入式特征选择。这三种方案我们其实都尝试过，过滤式特征选择主要是看特征与label之间的相关关系，有很多可以衡量的标准，例如Pearson相关系数、最大信息系数（MIC）等等。但是由于数据集比较稀疏，线性相关性都很低，MIC计算速度又非常慢（其实回想起来MIC应该坚持算完的），所以没有继续采用过滤式特征选择。包裹式特征选择就是根据模型的表现来决定使用哪些特征，我们当时采用的办法是用全特征跑一个模型，选出特征重要性（importance）排名靠前的一些特征入模型再跑，很遗憾，我们发现这样的做法始终无法超越全特征的得分，于是放弃。**当然，回想一下，这个步骤当时做的很不科学，应该做一个特征子集，然后加减特征跑迭代的，然后选出得分高于当前最高值的特征子集作为当前的特征。参见周志华《机器学习》第11章。** 嵌入式特征选择，不用说，模型中的L1正则已经帮我们做了这个事情了（稀疏超参数）。所以，因为不科学的特征选择做法，我们最后相当于没有做特征筛选。

​	特征工程的第二部分就是特征组合，我们对一些第二性征的检查例如：生殖器、乳腺等，做了交叉，得到了性别特征。由于其他数据脱敏以及缺乏医学专业知识，我们没能找到其他的组合方式。贪婪交叉特征的方法时间代价极大（详见：[我是链接](https://github.com/luoda888/tianchi-diabetes-top12)），我们也没有挑战人品随机组合。



```python
def get_division_feature(data,feature_name):
    new_feature = []
    new_feature_name = []
    for i in range(len(data[feature_name].columns)-1):
        for j in range(i+1,len(data[feature_name].columns)):
            new_feature_name.append(data[feature_name].columns[i] + '/' + data[feature_name].columns[j])
            new_feature_name.append(data[feature_name].columns[i] + '*' + data[feature_name].columns[j])
            new_feature_name.append(data[feature_name].columns[i] + '+' + data[feature_name].columns[j])
            new_feature_name.append(data[feature_name].columns[i] + '-' + data[feature_name].columns[j])
            new_feature.append(data[data[feature_name].columns[i]]/data[data[feature_name].columns[j]])
            new_feature.append(data[data[feature_name].columns[i]]*data[data[feature_name].columns[j]])
            new_feature.append(data[data[feature_name].columns[i]]+data[data[feature_name].columns[j]])
            new_feature.append(data[data[feature_name].columns[i]]-data[data[feature_name].columns[j]])
            
    
    temp_data = DF(pd.concat(new_feature,axis=1))
    temp_data.columns = new_feature_name
    data = pd.concat([data,temp_data],axis=1).reset_index(drop=True)
    
    print(data.shape)
    
    return data.reset_index(drop=True)

def get_square_feature(data,feature_name):
    new_feature = []
    new_feature_name = []
    for i in range(len(data[feature_name].columns)):
        new_feature_name.append(data[feature_name].columns[i] + '**2')
        new_feature_name.append(data[feature_name].columns[i] + '**1/2')
        new_feature.append(data[data[feature_name].columns[i]]**2)
        new_feature.append(data[data[feature_name].columns[i]]**(1/2))
        
    temp_data = DF(pd.concat(new_feature,axis=1))
    temp_data.columns = new_feature_name
    data = pd.concat([data,temp_data],axis=1).reset_index(drop=True)
    
    print(data.shape)
    
    return data.reset_index(drop=True)
```



## 模型选择

​	问题是一个回归问题，一般常用的方法有线性模型、树模型、深度学习。线性模型包括SVM、LR、BP网络等等，这些模型对输入数据非常敏感，前提是我们必须非常了解每个特征的分布（因为要做数据归一化），否则归一化后的特征可能反而会使得这类线性模型的表现更差，这也是为什么天池、kaggle这些比赛中获奖方案较少采用线性模型的原因。树模型一直都是天池、kaggle比赛的宠儿，说白了因为它调参简单，可以在完全不了解特征背后的分布和含义的情况下，直接丢进模型就能得到一个不错的结果。深度学习模型一般需要在较大的数据集上才能得到不错的结果，小数据集效果跟线性模型和树模型相差无几。

​	我们还是比较保守地选择了树模型，第一是我们作为新手，没有丰富的特征工程和调参经验，第二，相对而言我们比较熟悉树模型，前段时间刚看过一些树模型的算法原理，可以巩固平日所学。开始用的sklearn里面的GBDT Regression，但是sklearn的GBDT实现是串行执行，速度很慢，而且没有正则，也就无法做嵌入式特征筛选，而且目标函数只能选择rmse。当然用sklearn的GBDT效果也一般。之后尝试了陈天奇的神作xgboost，特征层面并行，给目标函数加入了l1、l2正则，近似分裂点寻找，可以自己修改目标函数（**我们把目标函数改成gamma regression后，线下得分有了很高的提升，所以后来就一直使用gamma regression作为目标函数。在看了初赛第一、第十二的开源方案后，发现他们的objective function的选择都是rmse，难道是我们的线下得分欺骗了我们？我现在不太记得有没有做过线上的比较了**），xgboost确实比sklearn快很多，但是这两个也都有个共同问题是无法自动处理类别特征（one-hot encoding）。所以稀疏特征会给这两种模型带来一些负面影响（1.花费精力重做类别特征。or 2.损失精度）。而lightgbm和catboost可以自动处理类别特征，而且lightgbm在并行计算上做的比xgboost更好，而且引入了leaf-wise的树生成策略提升精度，所以接下来我们尝试了lightgbm，也是我们初赛最高得分的单模型。

#### sklearn GBDT regression



#### xgboost

​	对比了一下我们的xgboost参数和第一名的xgboost参数，他们的xgboost的迭代次数远高于我们，学习率远低于我们，但是这里是有个权衡的，低学习率容易陷入局部最优，但若没有陷入局部最优，找到
的最优点会比高学习率找到的最优点更好，很遗憾，我没有在他们的代码里看到调参策略，因此目前还不知道他们的参数是如何确定的。另外我们的objective function如上文所说，选择了gamma 而他们仅仅
选择了linear（rmse）



我们的xgb参数：

```python
other_params = {'learning_rate': 0.1, 'n_estimators': i, 'max_depth': 6, 'min_child_weight': 1, 'seed': 0,\
				'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 1, 'reg_alpha': 0.1, 'reg_lambda': 1,\
				'objective':'reg:gamma'}
```
第一名的xgb参数：

```python
PARAMS = {
    'xgb': {
        'objective': 'reg:linear',
        'seed': RANDOM_SEED,
        'nthread': 56,
        'eta': 0.01,
        'min_child_weight': 10,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'silent': 1,
        'max_depth': 7,
        'num_rounds': 10000,
        'num_early_stop': 50
    }
```



#### lightgbm

同样，我对比了一下我们的lgb参数和第一名、第十二名的参数

这是我们的参数：

```python
model = lgb.LGBMRegressor(
    boosting_type='gbdt', 
    num_leaves=63, 
    max_depth=-1, 
    learning_rate=0.01, 
    n_estimators=1200, 
    subsample_for_bin=200000,
    objective='gamma', 
    class_weight=None,
    min_split_gain=0.01, 
    min_child_weight=0.001, 
    min_child_samples=20, 
    subsample=0.8, 
    subsample_freq=1, 
    colsample_bytree=1.0, 
    reg_alpha=1.0, 
    reg_lambda=1.0, 
    random_state=None, 
    n_jobs=4, 
    silent=True)
```

这是第一名的参数：

```python
'lgb': {
        'task': 'train',
        'boosting_type': 'gbdt',
        'objective': 'mse',
        'metric': 'l2',
        'num_leaves': 31,
        'learning_rate': 0.01,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 2,
        'verbose': 0,
        'bagging_seed': RANDOM_SEED,
        'num_rounds_lgb': 10000,
        'num_early_stop': 200
    }
```

这是第十二名的参数：

```python
params = {
    'learning_rate': 0.025,
    'boosting_type': 'gbdt',
    'objective': 'regression',
    'metric': 'mse',  # 使用均方误差
    'num_leaves': 60,  # 最大叶子数for base learner
    'feature_fraction': 0.6,  # 选择部分的特征
    'min_data': 100,  # 一个叶子上的最少样本数
    'min_hessian': 1,  # 一个叶子上的最小 hessian 和，子叶权值需大于的最小和
    'verbose': 1,
    'lambda_l1': 0.3,  # L1正则项系数
    'device': 'cpu',
    'num_threads': 8, #最好设置为真实核心数
}
```

​	可以看到同样的，我们的objective function选择不同，第一名甚至都没有做正则。。。不得不说特征为王！其他的参数大同小异，问题不是很大。

##### note

​	这里有个非常重要的参数，是我们复赛的时候才发现的，那就是每个模型的**basescore**，这个参数是模型求解过程中的一个初始值，默认0.5，但是如果我们可以把他调整到跟目标label值相近的值，模型可以更快收敛，相同的迭代次数下，模型能够找到更精确的最优解，在复赛的时候我们修改这个参数获得了二十名的提升。



## 建模策略

​	这一部分主要回顾总结一下我们在比赛过程中用到的一些建模策略，包括如何调参、如何做cv、如何做线下评价机制以及如何把我们的模型做成一个可持续提升分数的结构（最后这一点是我后来的一些思考）。

​	在初赛中，我们的策略主要是想做一个效果最好的单模型，因为单模型做到效果特别好的时候，后期再做模型融合的上升空间更大。据我们了解，模型融合能够带来的提升是少量的（对于我们的比赛而言，可能就是千分位或者万分位上的提高），而且无脑做模型融合而不去深挖特征是拣了芝麻丢了西瓜的行为，最终的结果也证明目前的机器学习比赛或者是项目都是特征为王的。

​	那么，单模型究竟怎么做呢？一开始，没有经验，为了评价模型的线下效果，我们直接把训练集拆成5份，用4份训练，1份线下评估，拆分的过程也没有设置随机数种子。这是一种非常不科学的做法，因为没有做cv，没有固定随机数种子，每次得到的模型都是不稳定的，更重要的是，这种做法等于直接丢掉了1份宝贵的训练数据，而且线下评估的时候也仅仅评估了模型的偏差，无法考量方差，所以无法评价模型的泛化能力。后来跟 怪物的姐姐 交流了一下，才得知正确的做法是做完整的5折cv然后把每一次的预测结果做一个平均融合，这样，线下评估就可以得到模型的方差和偏差也可以利用完整的数据集进行训练。下面给出这种方案的代码：

```python
def predict_ssy(train,test,kind = 'SSY'):
    train.reset_index(drop = True,inplace=True)
    target = kind
    IDcol = 'vid'
    predictors = [x for x in train.columns if x not in [target,IDcol]]
    cv_params = {'n_estimators': [600]}
    for i in cv_params['n_estimators']:
        model = lgb.LGBMRegressor(boosting_type='gbdt', num_leaves=63, max_depth=-1, 
                                  learning_rate=0.01, n_estimators=1200, 	                                                     subsample_for_bin=200000, objective='gamma',                                                     class_weight=None, min_split_gain=0.01, 
                                  min_child_weight=0.001, min_child_samples=20,                                                   subsample=0.8, subsample_freq=1,                                                                 colsample_bytree=1.0, reg_alpha=1.0, reg_lambda=1.0,                                             random_state=None, n_jobs=4, silent=True)
        kf = KFold(n_splits=5,shuffle=True,seed=29)#随机5折，设置随机种子
        X = train[predictors]
        y = train[target]
        score = []
        score1 = []
        count = 0
        final_result = pd.DataFrame()
        final_result['vid'] = test['vid']
        for train_index,test_index in kf.split(X):#做5折cv，加入了样本权重（lgb支持），每折后预测test集，结果保存在final_result[i]中，最后对每折的结果做平均，得到最终的结果。
            count = count + 1
            X_train, X_test = X.ix[train_index], X.ix[test_index]
            y_train, y_test = y.ix[train_index], y.ix[test_index]
            model.fit(X_train.drop('weight',axis=1), 
                      y_train,eval_set=[(X_test.drop('weight',axis=1),y_test)],
                      eval_metric='rmse',
                      early_stopping_rounds=2,
                      verbose=False,sample_weight=X_train['weight'])
            prediction = model.predict(X_test.drop('weight',axis=1))
            score.append(evalerror1(preds=prediction,labels=y_test))
            print 'the {0} {1}round Jcv: {2}'.format(kind,count,score[-1])
            prediction = model.predict(X_train.drop('weight',axis=1))
            score1.append(evalerror1(preds=prediction,labels=y_train))
            print 'the {0} round Jtrain: {1}'.format(kind,score1[-1])
            predictors1 = [x for x in test.columns if x not in [IDcol]]
            final_result[str(count)] = model.predict(test[predictors1])
    return final_result,score
```

​	可以看到我们的做法是：做5折cv，加入了样本权重（lgb支持），每折后预测test集，结果保存在final_result[i]中，最后对每折的结果做平均，得到最终的结果。每折的线下评估会保存在一个list中，这样最后可以得到5个线下得分，据此可以计算5次得分的均值得到模型的偏差，5次得分的方差得到模型的方差，据此评估模型的准确性和泛化能力。这就是我们的单模型建模思路以及线下评估机制。

​	接下来，我想讨论一下如何做一个可持续上分的建模策略，这一部分不涉及我们的具体工作（因为比赛中我们并没有做到这一点），只是自己的一些想法。

#### 上分过程

GBDT数字+粗糙处理文本特征特征：线下：0.02000，线上：0.03712，线下成绩比第一名还高，线上一测40名。后来发现是线下测评函数写错了log（x+1）写成了log（x+2）。

xgboost（objective=rmse）数字+粗糙处理文本特征：线下：0.032，线上：0.03560，线上线下差别明显减少，xgb的效果也确实比gbdt好。

xgboost（objective=gamma）数字+粗糙处理文本特征： 线下： ，线上：0.03411，对于这个数据集而言gamma分布回归效果要好于linear（rmse）。

lightgbm（objective=gamma）数字+粗糙处理文本特征：线下：，线上：0.03383，lgb号称是相比于xgboost损失了精度提升了效率，但是我们实际测试发现lgb应该是全方位超过了xgboost，原因应该是
leaf-wise的树生成策略弥补了最优分裂点寻找时损失的精度。

lightgbm（objective=gamma）数字+精细处理文本特征：线下： ，线上：0.03130，这里主要是对文本特征进行了提取工作，提取了脂肪肝、胰腺炎、脾脏健康程度等相关的特征。

lightgbm（objective=gamma）数字+精细处理文本特征+模型参数调整（**basescore非常重要**）：线下：，线上：0.03054。

lightgbm（objective=gamma）数字+精细处理文本特征+模型参数调整+组合特征：线下： ，线上：0.02947，在之前的基础上加入了组合特征：性别。

**notice**：这次的提交记录做的不是很好，以后需要对每次提交结果的特征、模型、参数、得分做更详细的记录。

​	第一次踩了很多坑，希望会对以后有所帮助。这些是总结出来的上分过程，实际的上分过程远远比这里叙述的曲折，譬如每次特征有优化，我们就要把之前的很多工作重复做一遍，浪费了很多时间和精力在重复的劳动上。如果特征的结构出现了较大改动，那可能之前搭好的模型里面需要调整很多内容（5折cv，5个label相当于是25个模型）。造成这个原因更多因为我们代码架构问题，总是想着快点做出结果所以没有很好地去思考如何做好整体的架构（当然这跟自己代码能力弱有很大关系）。下次做比赛需要了解一下pipeline这个东西，据说是专门用于模型之间的参数传递的，对于一些复杂模型，这个pipeline会比较有用。

#### 数据预处理class

​	另外，可以写一个数据预处理的class，参数传递采用整个数据集（前提是数据集不大，内存可以读的下），这样在整个class里面可以定义不同的处理方法（缺失值处理方法，labelencoder，onehot encoder等），每个方法输入值为整个数据集，输出值是改变后的数据集，如果需要修改特征，只需要增加方法再调用即可。然后还需要有一个训练集、测试集切分的方法，可以灵活快速地得到A榜、B榜的数据（这次比赛中我们这种机制其实做的已经不错了）。另外还要加入特征筛选的方法（过滤式、包裹式），以及对数据集的切分（为cross-validation服务）。

#### 可迭代的模型训练

​	这一块主要是对于模型的参数进行修改、模型融合，这次在比赛中发现了树模型中比较重要的参数有basescore、objective、eta、tree_num等，模型融合的方法主要有stacking、blending等。需要注意的是，当融合了很多个模型的时候，修改参数会变成一个非常繁琐的任务，因为众多模型融合的时候，已经没有办法用控制变量的方法去查看那些参数会对结果又提升（因为参数实在太多了）。所以我很好奇的一点是，有篇文章说kaggle比赛的第一名融合了上千个模型是如何做到的？我没有找到那个解决方案的开源代码，难道是不断融合结果.csv？那我是否可以做一个稳定模型后，把模型保存起来（用pickle 这个package），随着比赛的进行我们会积累各种各样的模型，最后把这些模型载入，对test集进行预测做平均融合，这样我们每一次新产生的特征或者模型都会在之前的基础上迭代，充分利用了之前的工作（这些工作的特征工程可能不尽相同，模型也不尽相同，所以融合可以得到不错的效果），可以形成一个可持续上分的训练机制。

​	最后说说模型融合：stacking、blending、平均融合。stacking容易出现数据泄露问题（其实就是过拟合，线下分数非常高，线上非常惨），但是优点是利用了所有训练数据集。blending的问题是有一部分训练集没有利用到，但是好处是不存在数据泄露问题。平均融合就是直接平均结果.csv，是现在天池这类竞赛强强联合最重要的一个手段，第二名和第三名把结果融合一下，可能分数就超过第一名了。stacking和blending的做法比平均融合要复杂的多，贴一下我们复赛做的stacking代码吧。

```python
def model_ground(kind='sys'):
    test_data_set = test_dataset_all_sys
    result = pd.DataFrame()
    new_feat = pd.DataFrame()
    result['vid'] = o.get_table(test_data_set,project='prj_tc_231654_144430_w9xa66').to_df().to_pandas()['vid']
    for i in range(1,k_fold+1):
        if (i==1) | (i==2):
            if kind == 'sys':
                baseScore = '100'
            elif kind == 'dia':
                baseScore = '70'
            else:
                baseScore = '0.5'
            model_name = 'ps_smart_wxc_'+kind+'_'+str(i)
            input_train_name = 'train_all_data_cv_0606_'+kind+'_'+str(i)
            input_test_name = 'test_all_data_cv_0606_'+kind+'_'+str(i)
            output_pred_name_cv = 'pssmart_cv_'+kind+str(i)
            output_pred_name = 'pssmart_pred_'+kind+str(i)
            o.execute_sql('drop table if exists '+output_pred_name+ ';')  #  同步的方式执行，会阻塞直到SQL执行完成
            o.execute_sql('drop table if exists '+output_pred_name_cv+ ';') 
            o.delete_offline_model(model_name,if_exists=True)
            if i == 1:
                parameters_train = {'objective':'reg:gamma','featureColNames':predictors,'metric':'rmse','treeCount':'1000','shrinkage':'0.02',
                                    'l1':'0','l2':'0','maxDepth':'5','baseScore':baseScore,
                                    'labelColName':kind,'inputTableName':'prj_tc_231654_144430_w9xa66.'+input_train_name,
                                    'modelName':r'algo_public/offlinemodels/'+model_name}
            elif i == 2:
                parameters_train = {'objective':'reg:gamma','featureColNames':predictors,'metric':'rmse','treeCount':'1200','shrinkage':'0.02',
                                    'l1':'0','l2':'0.5','maxDepth':'5','baseScore':'110',
                                    'labelColName':kind,'inputTableName':'prj_tc_231654_144430_w9xa66.'+input_train_name,
                                    'modelName':r'algo_public/offlinemodels/'+model_name}
            parameters_test = {'modelName':r'algo_public/offlinemodels/'+model_name,'inputTableName':input_test_name,
                               'outputTableName':output_pred_name_cv,'featureColNames':predictors,
                               'appendColNames':'vid','resultColName':kind+'_pred'}
            parameters_pred = {'modelName':r'algo_public/offlinemodels/'+model_name,'inputTableName':test_data_set,
                               'outputTableName':output_pred_name,'featureColNames':predictors,
                               'appendColNames':'vid','resultColName':kind+'_pred'}       
            # use 4 fold data train model                  
            inst_train = o.execute_xflow('ps_smart', 'algo_public',parameters=parameters_train)
            # get 1 fold test data predictions
            inst_predict = o.execute_xflow('prediction','algo_public',parameters=parameters_test)
            predict = o.get_table(output_pred_name_cv,project='prj_tc_231654_144430_w9xa66').to_df().to_pandas()
            new_feat = new_feat.append(predict)
            # get test predictions
            inst_predict_test = o.execute_xflow('prediction','algo_public',parameters=parameters_pred)
            result['result_pssmart_'+str(i)] = o.get_table(output_pred_name,project='prj_tc_231654_144430_w9xa66').to_df().to_pandas()[kind+'_pred']
        if (i==3) | (i==4):
            model_name = 'xgboost_wxc_'+kind+'_'+str(i)
            input_train_name = 'train_all_data_cv_0606_'+kind+'_'+str(i)
            input_test_name = 'test_all_data_cv_0606_'+kind+'_'+str(i)
            output_pred_name_cv = 'xgboost_cv_'+kind+str(i)
            output_pred_name = 'xgboost_pred_'+kind+str(i)
            o.execute_sql('drop table if exists '+output_pred_name+ ';')  #  同步的方式执行，会阻塞直到SQL执行完成
            o.execute_sql('drop table if exists '+output_pred_name_cv+ ';') 
            o.delete_offline_model(model_name,if_exists=True)
            if i == 3:
                parameters_train = {'eta':'0.01','num_round':'1200','featureColNames':predictors,'labelColName':kind,'max_depth':'8','colsample_bytree':'0.6','seed':'0',
                                    'objective':'reg:linear','eval_metric':'rmse','inputTableName':input_train_name,'modelName':model_name,'subsample':'0.8','gamma':'0',
                                    'lambda':'50'}
            elif i == 4:
                parameters_train = {'eta':'0.02','num_round':'1500','featureColNames':predictors,'labelColName':kind,'max_depth':'8','colsample_bytree':'0.6','seed':'0',
                                    'objective':'reg:linear','eval_metric':'rmse','inputTableName':input_train_name,'modelName':model_name,'subsample':'0.8','gamma':'0',
                                    'lambda':'50'}
            parameters_test = {'modelName':model_name,'inputTableName':input_test_name,
                               'outputTableName':output_pred_name_cv,'featureColNames':predictors,
                               'appendColNames':'vid','resultColName':kind+'_pred'}   
            parameters_pred = {'modelName':model_name,'inputTableName':test_data_set,
                               'outputTableName':output_pred_name,'featureColNames':predictors,
                               'appendColNames':'vid','resultColName':kind+'_pred'}      
            # use 4 fold data train model
            inst_train = o.execute_xflow('xgboost', 'algo_public',parameters=parameters_train)
            # get 1 fold test data predictions
            inst_predict = o.execute_xflow('prediction','algo_public',parameters=parameters_test)
            predict = o.get_table(output_pred_name_cv,project='prj_tc_231654_144430_w9xa66').to_df().to_pandas()
            new_feat = new_feat.append(predict)
            # get test predictions
            inst_predict_test = o.execute_xflow('prediction','algo_public',parameters=parameters_pred)
            result['result_xgb_'+str(i)] = o.get_table(output_pred_name,project='prj_tc_231654_144430_w9xa66').to_df().to_pandas()[kind+'_pred']
        if i == 5:
            model_name = 'GBDT_wxc_'+kind+'_'+str(i)
            input_train_name = 'train_all_data_cv_0606_'+kind+'_'+str(i)
            input_test_name = 'test_all_data_cv_0606_'+kind+'_'+str(i)
            output_pred_name_cv = 'GBDT_cv_'+kind+str(i)
            output_pred_name = 'GBDT_pred_'+kind+str(i)
            o.execute_sql('drop table if exists '+output_pred_name+ ';')  #  同步的方式执行，会阻塞直到SQL执行完成
            o.execute_sql('drop table if exists '+output_pred_name_cv+ ';') 
            o.delete_offline_model(model_name,if_exists=True)
            parameters_train = {'shrinkage':'0.02','lossType':'3','newtonStep':'1','metricType':'0','featureColNames':predictors,
                                'labelColName':kind,'maxDepth':'7','featureRatio':'1.0','randSeed':'2','minLeafSampleCount':'100',
                                'sampleRatio':'1.0','treeCount':'1000','inputTableName':input_train_name,'modelName':model_name}
            parameters_test = {'modelName':model_name,'inputTableName':input_test_name,
                               'outputTableName':output_pred_name_cv,'featureColNames':predictors,
                               'appendColNames':'vid','resultColName':kind+'_pred'}  
            parameters_pred = {'modelName':model_name,'inputTableName':test_data_set,
                               'outputTableName':output_pred_name,'featureColNames':predictors,
                               'appendColNames':'vid','resultColName':kind+'_pred'}     
            # use 4 fold data train model
            inst_train = o.execute_xflow('GBDT', 'algo_public',parameters=parameters_train)
            # get 1 fold test data predictions and make new feature
            inst_predict = o.execute_xflow('prediction','algo_public',parameters=parameters_test)
            predict = o.get_table(output_pred_name_cv,project='prj_tc_231654_144430_w9xa66').to_df().to_pandas()
            new_feat = new_feat.append(predict)
            # get test predictions
            inst_predict_test = o.execute_xflow('prediction','algo_public',parameters=parameters_pred)
            result['result_GBDT_'+str(i)] = o.get_table(output_pred_name,project='prj_tc_231654_144430_w9xa66').to_df().to_pandas()[kind+'_pred']    
    col = [x for x in result.columns if x not in ['vid']]
    result[kind] = result[col].mean(axis=1)
    print result
    return result[['vid',kind]],new_feat[['vid',kind+'_pred']]

def model_sec_floor(kind):
    if kind == 'sys':
        baseScore = 100
    elif kind == 'dia':
        baseScore = 70
    else:
        baseScore = 0.5
    model_name = 'ps_smart_wxc_'+kind+'_sec'
    input_train_name = 'train_all_data_newfeat_'+kind
    input_test_name = 'test_all_data_newfeat_'+kind
    output_pred_name = 'pssmart_pred_'+kind+'_final'
    o.execute_sql('drop table if exists '+output_pred_name+ ';')  #  同步的方式执行，会阻塞直到SQL执行完成
    o.delete_offline_model(model_name,if_exists=True)
    parameters_train = {'objective':'reg:gamma','featureColNames':predictors,'metric':'rmse','treeCount':'1000','shrinkage':'0.02',
                        'l1':'0','l2':'0','maxDepth':'5','baseScore':baseScore,
                        'labelColName':kind,'inputTableName':'prj_tc_231654_144430_w9xa66.'+input_train_name,
                        'modelName':r'algo_public/offlinemodels/'+model_name}
    parameters_test = {'modelName':r'algo_public/offlinemodels/'+model_name,'inputTableName':input_test_name,
                       'outputTableName':output_pred_name,'featureColNames':predictors,
                       'appendColNames':'vid','resultColName':kind+'_pred'}      
    # use all data train model                  
    inst_train = o.execute_xflow('ps_smart', 'algo_public',parameters=parameters_train)
    # get final predictions
    inst_predict = o.execute_xflow('prediction','algo_public',parameters=parameters_test)
    final_result = o.get_table(output_pred_name,project='prj_tc_231654_144430_w9xa66').to_df().to_pandas()
    return final_result[['vid',kind+'_pred']]
```

​	这仅仅是两层的stacking，如果要做更多层的融合，需要写一个class来做了，不然代码太难维护了，之前在GitHub上看到过一个这样的实例，后面会贴出来。



## 资料整理

blending实现方案：[blending](https://github.com/vandesa003/vertebral/blob/master/stacked_generalization.py)

stacking实现方案：可以自己做。

平均融合实现方案：没有链接，只有代码。

贪婪特征组合方案：[贪婪特征组合](https://github.com/vandesa003/tianchi-diabetes-top12)

# 复赛

​	复赛的思路跟初赛差不多，加入了基因序列，但是相关性好像不大。数据量骤减到3000个样本。主要讲一下阿里的机器学习平台吧。平台限制很多，只有GBDT、RF、PS-SMART（阿里自己开发的树模型，比较类似lgb）、Xgboost（因为版权问题，平台隐藏了，但是可以用PAI命令调用，然而没有文档，仅靠之前的一个前辈的帖子加自己摸索），可以安装外部package，前提是纯python的package，不能调用C++接口。具体平台的使用方法可以直接查看官方文档。平台提供了pyodps的python接口，可以用python写所有的代码，其他没什么好说的了，可以直接看代码（我们还找到了ps-smart的python接口的一个bug，还好最后官方给了替代的解决方案）。

## SMOT

复赛的数据量太少了，模型非常容易过拟合导致线上成绩很不理想。因此我们用SMOT做了数据集扩充。具体的方案是这样的：

| sample      | feat1       | feat2       | label |
| ----------- | ----------- | ----------- | ----- |
| 1           | *feat1_1*   | *feat2_1*   | 1     |
| 1           | **feat1_2** | **feat2_2** | 1     |
| fake sample | *feat1_1*   | **feat2_2** | 1     |

​	通过交换相同label的样本的不同特征，可以得到一个新的人造样本fake sample。通过这样的做法我们把3000数据集扩充到了4000。但是很遗憾线上并没有带来提升，但是我依然觉得这是小数据集上的一个可行的方案。

​	另外还有一些过采样的方案可以应用在小数据集上，但是过采样方案只是单纯的复制样本，更容易造成不平衡的数据集，因此我们觉得没有SMOT合理，没有尝试。

# 总结

1. 数据分布很重要，不管是特征的分布还是label的分布。有时候我们判断线上分数是否会上升就是直接从预测数据的分布入手的。
2. 模型参数中的basescore很重要，初赛的时候一直忽视了这个点（跟我们迭代次数少，学习率较大可能也有关系）。
3. 特征需要做更细化的处理，我们的模型都采用了跟前几名相同的模型，参数也都大同小异，在跟一些成绩不错的选手交流后，发现他们的特征工程都做的非常细致，而我们的特征做的有点粗糙，这点应该是我们后期被卡住的主要原因（重做特征时间成本有点大，就没有再做了）。如果下次做比赛，一定要把重心放在特征工程上。

# 写在最后

​	这两个月过得还是挺充实的，在比赛的过程中学习到了很多知识，也认识了很多大佬。其实非常感谢我的两个靠谱队友（茹宝、师兄）。一起刷了2次夜，开了无数次会，写了无数行代码，队内的气氛都非常欢乐。自己会觉得有点愧疚没有能拿到更好的成绩，对不起大家两个月以来的付出。不过，时间还有很多，剩下的一年里足够野蛮生长，成为大佬。希望我们都能从这次比赛中收获自己想要的东西，
	结果嘛，不是太重要~